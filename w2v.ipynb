{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Use only NumPy’s RNG to avoid any name collisions\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define your word categories\n",
    "word_categories = {\n",
    "    'animals':    ['cat', 'dog', 'lion', 'tiger', 'elephant', 'giraffe', 'zebra', 'monkey'],\n",
    "    'fruits':     ['apple', 'banana', 'orange', 'mango', 'grape', 'strawberry', 'kiwi', 'pineapple'],\n",
    "    'colors':     ['red', 'blue', 'green', 'yellow', 'purple', 'orange', 'pink', 'black'],\n",
    "    'emotions':   ['happy', 'sad', 'angry', 'excited', 'calm', 'nervous', 'joyful', 'anxious'],\n",
    "    'weather':    ['sunny', 'rainy', 'cloudy', 'windy', 'stormy', 'foggy', 'snowy', 'clear']\n",
    "}\n",
    "\n",
    "def generate_training_data(window_size=2, num_samples=1000):\n",
    "    \"\"\"\n",
    "    Generates (context_words, center_word) pairs.\n",
    "    Uses NumPy for random sampling to avoid any clashes with a shadowed 'random'.\n",
    "    \"\"\"\n",
    "    categories = list(word_categories.keys())\n",
    "    n_cats = len(categories)\n",
    "    data = []\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        # pick a random category index\n",
    "        cat_idx = np.random.randint(0, n_cats)\n",
    "        words = word_categories[categories[cat_idx]]\n",
    "        \n",
    "        # ensure we can pick a center word with full window on both sides\n",
    "        min_i = window_size\n",
    "        max_i = len(words) - window_size - 1\n",
    "        center_i = np.random.randint(min_i, max_i + 1)\n",
    "        \n",
    "        # slice out the context words\n",
    "        left  = words[center_i - window_size : center_i]\n",
    "        right = words[center_i + 1 : center_i + window_size + 1]\n",
    "        context = left + right\n",
    "        \n",
    "        center_word = words[center_i]\n",
    "        data.append((context, center_word))\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Generate the data\n",
    "training_data = generate_training_data()\n",
    "\n",
    "# Show a few examples\n",
    "print(\"Some training examples (context → center):\")\n",
    "for ctx, ctr in training_data[:5]:\n",
    "    print(f\"  {ctx} → {ctr}\")\n",
    "\n",
    "# Build vocabulary\n",
    "all_words = [w for ctx, ctr in training_data for w in ctx + [ctr]]\n",
    "vocab = sorted(set(all_words))\n",
    "\n",
    "# Create mappings\n",
    "word_to_index = {w: i for i, w in enumerate(vocab)}\n",
    "index_to_word = {i: w for w, i in word_to_index.items()}\n",
    "\n",
    "print(\"\\nVocabulary size:\", len(vocab))\n",
    "print(\"\\nSample mappings:\")\n",
    "for w, idx in list(word_to_index.items())[:5]:\n",
    "    print(f\"  {w}: {idx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_to_index)\n",
    "print(index_to_word)\n",
    "print(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a dictionary to store word appearances\n",
    "word_appearances = {}\n",
    "\n",
    "\n",
    "# Iterate through training data to record word positions\n",
    "for index,word in enumerate(vocab):\n",
    "    list=np.zeros(len(vocab))\n",
    "    list[index] = 1\n",
    "    word_appearances[word] = list\n",
    "   \n",
    "\n",
    "print(word_appearances)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_data)\n",
    "len(training_data)\n",
    "\n",
    "def convert_to_onehot(training_data,vocab,word_appearances):\n",
    "    onehotcoded_data = np.zeros((len(vocab),len(training_data)))\n",
    "    onehotcoded_y=np.zeros((len(vocab),len(training_data)))\n",
    "    for i in range(len(training_data)):\n",
    "        temp=np.zeros(len(vocab))\n",
    "        temp_y=np.zeros(len(vocab))\n",
    "        for j in range(len(training_data[i][0])):\n",
    "            \n",
    "            temp+=word_appearances[training_data[i][0][j]]\n",
    "            \n",
    "        temp_y+=word_appearances[training_data[i][1]]\n",
    "      \n",
    "        onehotcoded_y[:,i]=temp_y\n",
    "       \n",
    "        onehotcoded_data[:,i]=temp\n",
    "    \n",
    "    \n",
    "    return onehotcoded_data,onehotcoded_y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec:\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.W = np.random.randn(embedding_dim, vocab_size)\n",
    "        self.W_ = np.random.randn(vocab_size, embedding_dim)\n",
    "        self.b=np.zeros((embedding_dim,1))\n",
    "        self.b_=np.zeros((vocab_size,1))\n",
    "        \n",
    "   \n",
    "        \n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        self.Z = np.dot(self.W,self.X)+self.b\n",
    "        self.Y = np.dot(self.W_,self.Z)+self.b_\n",
    "        self.prediction = self.softmax(self.Y)\n",
    "        \n",
    "        \n",
    "    def softmax(self,x):\n",
    "        x_shifted=x-np.max(x)\n",
    "        return np.exp(x_shifted)/np.sum(np.exp(x_shifted))\n",
    "\n",
    "\n",
    "    def cross_entropy_cost(self,y,y_hat):\n",
    "          epsilon = 1e-15\n",
    "          y_hat = np.clip(y_hat, epsilon, 1 - epsilon)\n",
    "          cost = -np.sum(y*np.log(y_hat))/y.shape[1]\n",
    "          return cost\n",
    "\n",
    "    def backward(self,y,learning_rate):\n",
    "       \n",
    "        self.dZ = self.prediction - y\n",
    "        self.dW_ = np.dot(self.dZ,self.Z.T)/self.dZ.shape[1]\n",
    "        self.db=np.sum(self.dZ,axis=1,keepdims=True)/self.dZ.shape[1]\n",
    "        dZ = np.dot(self.W_.T, self.dZ)\n",
    "        self.dW = np.dot(dZ,self.X.T)/self.dZ.shape[1]\n",
    "        self.db_=np.sum(self.dZ,axis=1,keepdims=True)/self.dZ.shape[1]\n",
    "        self.W = self.W - self.dW*learning_rate\n",
    "        self.W_ = self.W_ - self.dW_*learning_rate\n",
    "        \n",
    "        # Should be (5,3) but is (5,5)\n",
    "        \n",
    "        \n",
    "   \n",
    "    def train(self,X,y,learning_rate,epochs):\n",
    "        for i in range(epochs):\n",
    "            \n",
    "            \n",
    "           \n",
    "            self.forward(X)\n",
    "            cost=self.cross_entropy_cost(y,self.prediction)\n",
    "            print(f\"Cost: {cost}\")\n",
    "            self.backward(y,learning_rate)\n",
    "            print(f\"Epoch {i+1} completed\")\n",
    "    def predict(self,X):\n",
    "        self.forward(X)\n",
    "        return self.prediction\n",
    "    def get_embedding(self,word,vocab):\n",
    "        index = vocab.index(word)\n",
    "        print(f\"Embedding for {word}: {self.W[:,index]}\")\n",
    "        return self.W[:,index]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=convert_to_onehot(training_data,vocab,word_appearances)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.T, y.T, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Update the model training to use the training split\n",
    "X_train=X_train.T\n",
    "y_train=y_train.T\n",
    "model = Word2Vec(vocab_size=len(vocab), embedding_dim=3)\n",
    "\n",
    "model.train(X_train, y_train, learning_rate=0.0001, epochs=1000)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.get_embedding(\"angry\",vocab)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
